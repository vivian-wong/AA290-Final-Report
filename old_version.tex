%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} 
\usepackage{algpseudocode,algorithm}
\usepackage{bm}
\usepackage{color}
\usepackage{subfig}
\usepackage{url}
\usepackage{floatrow}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{\LARGE \bf
A Reinforcement Learning Approach â€¨for Rebalancing Vehicles in Autonomous Mobility-on-Demand Systems
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Vivian Wong% <-this % stops a space
\thanks{Vivian Wong is with the Department of Civil and Environmental Engineering, Stanford University, Stanford, CA, 94305:
        {\tt\small vwwong3@stanford.edu}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This paper presents an approach to rebalance vehicles in Autonomous Mobility-on-Demand systems (AMoD) through a learning and a rebalancing stage. In the learning stage, we use reinforcement learning on historical to approximate the values of each autonomous vehicle's actions. Then in the rebalancing stage, we use these values to find advantageous rebalancing actions in real-time for each available vehicle. We test this approach on real customer data from Didi Chuxing: we show that the computational complexity of this approach does not increase with the number of stations in the system, however, sacrificing [TO DO: insert percent suckiness] in performance compared with a reactive real-time rebalancing strategy.  

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

As our cities grow rapidly everyday, transportation becomes a challenge for many urban dwellers. Among many future mobility systems, autonomous mobility on demand (AMoD) is deemed one of the most promising modes of transportation. [TO DO: COMPLETE INTRODCUTION for A. LITERATURE, B. MOTIVATION AND C. STATEMENT OF WORK]

\subsection{Literature}

Provide here a thorough literature review for your problem (about 10-12 papers).  I would like to see your judgment about each paper you cite (e.g., this paper does not address in an efficient way the case with...).

\subsection{Motivation of proposed work} 
In light of the previous literature review, explain why your project is important.

\subsection{Statement of work} 
State the objectives of your project at a general level.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem formulation}
\subsection{Background Information}
Consider an environment discretized into a set of $\mathcal{N}$ stations (also can be viewed as discretized regions, as in [1]). Time is presented in discrete intervals of unit time $1$. The furthest time that will be considered is denoted as $T$.

Furthermore, let $M$ denote the total number of vehicles. $v_i$ denotes the number of idle vehicles, or vehicles waiting at station $i\in N$ and not serving any customer at time $t$. $v_{it'}$ denotes the number of vehicles that will arrive at station $i$, and will become an idle vehicle at $t'$. Note that we assume that the vehicles will arrive at time $t'$ deterministically, or that no uncertainty is taken into account for travel times between two stations. 

In an AMoD system, at every station there are customers demanding vehicles to service their transportation requests. We denote $\lambda_{ijt}$ as the number of customers waiting to get a vehicle at station $i$ at time $t$, wishing to travel to station $j$. $\lambda'_{ijt}$ represents the demand forecasted with a LSTM neural network, as described in [1].
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MDP Model Description}
At every time step $t \in \{1,...,T-1,T\}$, a MDP, an \textit{agent} observes the environment and store the values of interest in the observation as its \textit{current state, $s_t$}. The agent then conducts an \textit{action $a_t$} following a certain policy. At the end of this time step (or the beginning of time step $t+1$), a \textit{new state $s_{t+1}$} and a \textit{reward $r_t$} are observed. The definitions of MDP components are:\\

\noindent \textit{Agent}: Each vehicle is treated as an agent. Although this definition makes the problem a multiagent problem, it reduces the dimensions of actions, so that there is a smaller action space to explore. \\

\noindent \textit{State, $s_t$}: The state at time step $t$ is defined as a vector of length $2|N|$, concatenated with two $|N|$ dimensional vectors, $v_{excess}$, [TO DO: define vexcess] and $O$ [TO DO: define origin vector]. 

\noindent \textit{Action, $a_t$}: An agent is only made to choose an action when it is idle. Consider an idle agent currently at station $i$. Its action is defined as $j$, where $j\in \mathcal{N}$. If $i = j$, then the agent remains idle at its current station. If $i \neq j$, the agent departs to go to station $j$. The dimension of the action space is therefore $\mathcal{N}$. \\

\noindent \textit{Reward, $r_t$}: The reward is equal to the average customer wait time at time $t$. [TO DO: DEFINE FINAL REWARD DEFINITION] \\ 

Lastly, from classical MDP definitions, we define the state-action value, $Q(s,a)$ as the expected utility if following an action $a$ from state $t$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Formulation}
\subsubsection{Learning Stage}
The goal in the learning stage is to output accurate estimation of the state-action values, $Q(s_t,a_t)$ for each explored state.

To update and estimate state-action values, we will use the Q-learning algorithm:
\begin{algorithm}[H]
\caption{Q-Learning}
\begin{algorithmic}
\For{each ($s_t,a_t,r_t,s_{t+1}$)}
    \State $a_{t+1} = \pi(s_{t+1})$
    \State $Q_{target} = r_t+\gamma \max_{a_{t+1}} Q(s_{t+1},a_{t+1})$
    \State $Q_{prediction} = Q(s_t,a_t) \leftarrow (1-\alpha) Q(s_t,a_t)+\alpha Q_{target}$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{SARSA with VFA}
\begin{algorithmic}
\For{each ($s_t,a_t,r_t,s_{t+1}$)}
    \State $a_{t+1} = \pi(s_{t+1})$
    \State $Q_{target} = r_t+\gamma  Q(s_{t+1},a_{t+1})$
    \State  $Q_{prediction} = Q(s_t,a_t) \leftarrow (1-\alpha) Q(s_t,a_t)+\alpha Q_{target}$
    \State J = $(Q_{target} - Q_{prediction})^2$
    \State $\textbf{w} \leftarrow \textbf{w} - \alpha \nabla_{ \textbf{w}} J$
\EndFor
\end{algorithmic}
\end{algorithm}


The algorithm approaches convergence as the following objective is met:
\begin{equation}
        \min J(s,a;\textbf{w}) = (Q_{target} - Q_{prediction})^2\\
        Q_{target} = w x(s)[a]\\
        Q_{predict} = r + \gamma 
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Rebalancing Stage}
Actions that give highest state-action values for each agent may be only optimal locally but not globally. At this stage, we aim to use the state-action values learned in the learning stage to find actions that minimize customer wait time by rebalancing empty vehicles. 

To do this, we find the global optimum by assigning actions sequentially for each vehicle. We create smaller simulated states, $\hat{s_t}$ that are updated every time a vehicle has been assigned an action. The algorithm follows: 
\begin{algorithm}[H]
\caption{Sequentially Assigning Actions}
\begin{algorithmic}
\For{each time step $t \in {0,1,...,T}$}
    \State $\hat{s_t}=s_t$
    \For{each vehicle $m$ in $v_i_t$ vehicles}
        \If {vehicle is idle}
            \State $a_t^m \leftarrow \argmax_a(Q(\hat{s_t},a))$
            \State $\hat{s_t}$ \leftarrow \textbf{simulate}$(a^m_t,s_t)$
        \EndIf
    \EndFor
    \State\textbf{Assign} $\{a^1_t,a^2_t,...,a^M_t\}$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Probabilistically Assigning Actions}
\begin{algorithmic}
\For{each time step $t \in {0,1,...,T}$}
    \For{each vehicle $m$ in $v_i_t$ vehicles}
        \If {vehicle is idle}
            \State $P_j = \frac{Q(s_t,j)}{\sum_a Q(s_t,a)}$
            \State $a_t^m \leftarrow j$ with probability $P_j$
        \EndIf
    \EndFor
    \State\textbf{Assign} $\{a^1_t,a^2_t,...,a^M_t\}$
\EndFor
\end{algorithmic}
\end{algorithm}

This algorithm finds actions that aim for the following objective: 
\begin{equation}
        \min -\sum_m^M Q^m(s,a)\forall i,j\in \mathcal{N}
\end{equation}
By creating smaller simulated states and thereby making decisions sequentially, we can take into account of the actions of other agents while optimizing actions for one agent.  

\section{Proposed Solution}

\section{Simulation/Experiments}

\addtolength{\textheight}{-3cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

\subsection{Conclusions}

\bibliographystyle{unsrt} 
\bibliography{Biblio}  

\section*{Appendix --- style guidelines}

\subsection{Figures and Tables}

Position figures and tables at the tops and bottoms of columns.
Avoid placing them in the middle of columns. Large figures and tables
may span across both columns. Figure captions should be below the figures;
 table captions should be above the tables. Avoid placing figures and tables
  before their first mention in the text. Use the abbreviation ``Fig. 1'',
  even at the beginning of a sentence.
Figure axis labels are often a source of confusion.
Try to use words rather then symbols. As an example write the quantity ``Inductance",
 or ``Inductance L'', not just.
 Put units in parentheses. Do not label axes only with units.
 In the example, write ``Inductance (mH)'', or ``Inductance L (mH)'', not just ``mH''.
 Do not label axes with the ratio of quantities and units.
 For example, write ``Temperature (K)'', not ``Temperature/K''.

\subsection{Numbering}

Number reference citations consecutively in square brackets \cite{Garcia.ea:Auto89}.
 The sentence punctuation follows the brackets \cite{Garcia.ea:Auto89}.
 Refer simply to the reference number, as in \cite{Garcia.ea:Auto89}.
 Do not use ``ref. \cite{Garcia.ea:Auto89}'' or ``reference \cite{Garcia.ea:Auto89}''.
Number footnotes separately in superscripts\footnote{This is a footnote}
Place the actual footnote at the bottom of the column in which it is cited.
Do not put footnotes in the reference list.
Use letters for table footnotes (see Table I).

\subsection{Abbreviations and Acronyms}

Define abbreviations and acronyms the first time they are used in the text,
even after they have been defined in the abstract. Abbreviations such as
IEEE, SI, CGS, ac, dc, and rms do not have to be defined. Do not use
abbreviations in the title unless they are unavoidable.

\subsection{Equations}

Number equations consecutively with equation numbers in parentheses flush
 with the right margin, as in (1). To make your equations more compact
 you may use the solidus (/), the exp. function, or appropriate exponents.
  Italicize Roman symbols for quantities and variables, but not Greek symbols.
   Use a long dash rather then hyphen for a minus sign. Use parentheses to avoid
    ambiguities in the denominator.
Punctuate equations with commas or periods when they are part of a sentence:
$$\Gamma_2 a^2 + \Gamma_3 a^3 + \Gamma_4 a^4 + ... = \lambda \Lambda(x),$$
where $\lambda$ is an auxiliary parameter.

Be sure that the symbols in your equation have been defined before the
equation appears or immediately following.
Use ``(1),'' not ``Eq. (1)'' or ``Equation (1),''
except at the beginning of a sentence: ``Equation (1) is ...''.

%\begin{table}
%\caption{An Example of a Table}
%\label{table_example}
%\begin{center}
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}

   \begin{figure}[thpb]
      \centering
      %\includegraphics[scale=1.0]{figurefile.jpg}
      \caption{Inductance of oscillation winding on amorphous
       magnetic core versus DC bias magnetic field}
      \label{figurelabel}
   \end{figure}




\end{document}
